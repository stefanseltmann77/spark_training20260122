{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T13:59:13.597863Z",
     "start_time": "2026-01-30T13:58:40.538037200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "spark.catalog.setCurrentCatalog('workspace')\n",
    "spark.catalog.setCurrentDatabase('default')\n"
   ],
   "id": "e9d8dc97dd980e5b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Databricks Connect detected\n",
      "Databricks Connect detected\n",
      "+-----------------------+\n",
      "|'dbconnect established'|\n",
      "+-----------------------+\n",
      "|dbconnect established  |\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T13:59:13.639478600Z",
     "start_time": "2026-01-30T13:59:13.607275100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CONFIGS: please adjust to your volume\n",
    "PATH_DATASETS = Path('/Volumes/workspace/default/data')"
   ],
   "id": "8b2f2e107e6f9d7f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T13:21:27.939434400Z",
     "start_time": "2026-01-30T13:21:17.651647400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# recipe data\n",
    "df_recipe = spark.read.csv((path_demodata / 'recipeData.csv').as_posix(), header=True, inferSchema=True)\n",
    "df_recipe = df_recipe.dropna().withColumnRenamed('Size(L)', 'Size')\n",
    "\n",
    "df_recipe.write.saveAsTable('fct_recipe', mode=\"overwrite\", overwriteSchema=True)\n",
    "df_style = df_recipe.select(\"StyleID\", \"Style\").distinct()\n",
    "df_style.write.saveAsTable('dim_style', mode=\"overwrite\", overwriteSchema=True)\n",
    "\n"
   ],
   "id": "10c245034730a6da",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T13:58:27.961211200Z",
     "start_time": "2026-01-30T13:58:27.689140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# fraud data\n",
    "csv_reader = spark.read.format('csv').options(header='true', inferSchema='true')\n",
    "df_fraud = csv_reader.load((path_demodata / 'bs140513_032310.csv').as_posix())\n",
    "df_fraud.write.saveAsTable('bs140513_032310', mode='overwrite')\n"
   ],
   "id": "c4641e3f99dc00b5",
   "outputs": [
    {
     "ename": "SparkConnectException",
     "evalue": "[NO_ACTIVE_SESSION] No active Spark session found. Please create a new Spark session before running the code.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mSparkConnectException\u001B[39m                     Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[8]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      2\u001B[39m csv_reader = spark.read.format(\u001B[33m'\u001B[39m\u001B[33mcsv\u001B[39m\u001B[33m'\u001B[39m).options(header=\u001B[33m'\u001B[39m\u001B[33mtrue\u001B[39m\u001B[33m'\u001B[39m, inferSchema=\u001B[33m'\u001B[39m\u001B[33mtrue\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m      3\u001B[39m df_fraud = csv_reader.load((path_demodata / \u001B[33m'\u001B[39m\u001B[33mbs140513_032310.csv\u001B[39m\u001B[33m'\u001B[39m).as_posix())\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m \u001B[43mdf_fraud\u001B[49m\u001B[43m.\u001B[49m\u001B[43mwrite\u001B[49m\u001B[43m.\u001B[49m\u001B[43msaveAsTable\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mbs140513_032310\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43moverwrite\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\sparktraining_dbconnect\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\readwriter.py:737\u001B[39m, in \u001B[36mDataFrameWriter.saveAsTable\u001B[39m\u001B[34m(self, name, format, mode, partitionBy, **options)\u001B[39m\n\u001B[32m    735\u001B[39m \u001B[38;5;28mself\u001B[39m._write.table_name = name\n\u001B[32m    736\u001B[39m \u001B[38;5;28mself\u001B[39m._write.table_save_method = \u001B[33m\"\u001B[39m\u001B[33msave_as_table\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m737\u001B[39m _, _, ei = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_spark\u001B[49m\u001B[43m.\u001B[49m\u001B[43mclient\u001B[49m\u001B[43m.\u001B[49m\u001B[43mexecute_command\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    738\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_write\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_spark\u001B[49m\u001B[43m.\u001B[49m\u001B[43mclient\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_write\u001B[49m\u001B[43m.\u001B[49m\u001B[43mobservations\u001B[49m\n\u001B[32m    739\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    740\u001B[39m \u001B[38;5;28mself\u001B[39m._callback(ei)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\sparktraining_dbconnect\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\client\\core.py:1589\u001B[39m, in \u001B[36mSparkConnectClient.execute_command\u001B[39m\u001B[34m(self, command, observations, extra_request_metadata)\u001B[39m\n\u001B[32m   1587\u001B[39m     req.user_context.user_id = \u001B[38;5;28mself\u001B[39m._user_id\n\u001B[32m   1588\u001B[39m \u001B[38;5;28mself\u001B[39m._set_command_in_plan(req.plan, command)\n\u001B[32m-> \u001B[39m\u001B[32m1589\u001B[39m data, _, metrics, observed_metrics, properties = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_execute_and_fetch\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1590\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobservations\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_request_metadata\u001B[49m\n\u001B[32m   1591\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1592\u001B[39m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n\u001B[32m   1593\u001B[39m ei = ExecutionInfo(metrics, observed_metrics)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\sparktraining_dbconnect\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\client\\core.py:2139\u001B[39m, in \u001B[36mSparkConnectClient._execute_and_fetch\u001B[39m\u001B[34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[39m\n\u001B[32m   2136\u001B[39m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] = {}\n\u001B[32m   2138\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers=\u001B[38;5;28mself\u001B[39m._progress_handlers, operation_id=req.operation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[32m-> \u001B[39m\u001B[32m2139\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_execute_and_fetch_as_iterator\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2140\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobservations\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_request_metadata\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprogress\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprogress\u001B[49m\n\u001B[32m   2141\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   2142\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mStructType\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   2143\u001B[39m \u001B[43m            \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\sparktraining_dbconnect\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\client\\core.py:2115\u001B[39m, in \u001B[36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[39m\u001B[34m(self, req, observations, extra_request_metadata, progress)\u001B[39m\n\u001B[32m   2113\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[32m   2114\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[32m-> \u001B[39m\u001B[32m2115\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_handle_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43merror\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\sparktraining_dbconnect\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\client\\core.py:2434\u001B[39m, in \u001B[36mSparkConnectClient._handle_error\u001B[39m\u001B[34m(self, error)\u001B[39m\n\u001B[32m   2432\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc.RpcError):\n\u001B[32m   2433\u001B[39m         \u001B[38;5;28mself\u001B[39m._handle_rpc_error(error)\n\u001B[32m-> \u001B[39m\u001B[32m2434\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[32m   2435\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m   2436\u001B[39m     \u001B[38;5;28mself\u001B[39m.thread_local.inside_error_handling = \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\sparktraining_dbconnect\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\client\\core.py:2093\u001B[39m, in \u001B[36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[39m\u001B[34m(self, req, observations, extra_request_metadata, progress)\u001B[39m\n\u001B[32m   2088\u001B[39m metadata = \u001B[38;5;28mself\u001B[39m.metadata() + extra_request_metadata  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[32m   2089\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._use_reattachable_execute:\n\u001B[32m   2090\u001B[39m     \u001B[38;5;66;03m# Don't use retryHandler - own retry handling is inside.\u001B[39;00m\n\u001B[32m   2091\u001B[39m     generator = ExecutePlanResponseReattachableIterator(\n\u001B[32m   2092\u001B[39m         req,\n\u001B[32m-> \u001B[39m\u001B[32m2093\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_stub\u001B[49m,\n\u001B[32m   2094\u001B[39m         \u001B[38;5;28mself\u001B[39m._retrying,\n\u001B[32m   2095\u001B[39m         metadata,\n\u001B[32m   2096\u001B[39m         \u001B[38;5;28mself\u001B[39m._send_release_until,\n\u001B[32m   2097\u001B[39m     )\n\u001B[32m   2098\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   2099\u001B[39m         \u001B[38;5;28;01mfor\u001B[39;00m b \u001B[38;5;129;01min\u001B[39;00m generator:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\sparktraining_dbconnect\\.venv\\Lib\\site-packages\\pyspark\\sql\\connect\\client\\core.py:1015\u001B[39m, in \u001B[36mSparkConnectClient._stub\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1012\u001B[39m \u001B[38;5;129m@property\u001B[39m\n\u001B[32m   1013\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_stub\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> grpc_lib.SparkConnectServiceStub:\n\u001B[32m   1014\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.is_closed:\n\u001B[32m-> \u001B[39m\u001B[32m1015\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectException(\n\u001B[32m   1016\u001B[39m             errorClass=\u001B[33m\"\u001B[39m\u001B[33mNO_ACTIVE_SESSION\u001B[39m\u001B[33m\"\u001B[39m, messageParameters=\u001B[38;5;28mdict\u001B[39m()\n\u001B[32m   1017\u001B[39m         ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1018\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._internal_stub\n",
      "\u001B[31mSparkConnectException\u001B[39m: [NO_ACTIVE_SESSION] No active Spark session found. Please create a new Spark session before running the code."
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T13:21:51.736772600Z",
     "start_time": "2026-01-30T13:21:28.755208200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# book data\n",
    "csv_reader = spark.read.format('csv').options(header='true', inferSchema='true')\n",
    "df_books = csv_reader.options(**{\"escape\": \"\\\"\", \"quote\": \"\\\"\"}).load((path_demodata / 'books.csv').as_posix())\n",
    "df_book_tags = csv_reader.load((path_demodata / 'book_tags.csv').as_posix())\n",
    "df_ratings = csv_reader.load((path_demodata / 'ratings.csv').as_posix())\n",
    "df_tags = csv_reader.load((path_demodata / 'tags.csv').as_posix())\n",
    "df_toread = csv_reader.load((path_demodata / 'to_read.csv').as_posix())\n",
    "\n",
    "df_books.write.saveAsTable('books', overwriteSchema=True, mode=\"overwrite\")\n",
    "df_book_tags.write.saveAsTable('book_tags', overwriteSchema=True, mode=\"overwrite\")\n",
    "df_ratings.write.saveAsTable('ratings', overwriteSchema=True, mode=\"overwrite\")\n",
    "df_tags.write.saveAsTable('tags', overwriteSchema=True, mode=\"overwrite\")\n",
    "df_toread.write.saveAsTable('to_read', overwriteSchema=True, mode=\"overwrite\")\n"
   ],
   "id": "86ceb0af85a77980",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T13:21:59.370556Z",
     "start_time": "2026-01-30T13:21:53.569718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# optional add keys\n",
    "spark.sql(\"ALTER TABLE fct_recipe ALTER COLUMN BeerID SET NOT NULL;\")\n",
    "spark.sql(\"ALTER TABLE fct_recipe ADD CONSTRAINT beer_pk PRIMARY KEY (BeerID) RELY\")\n",
    "spark.sql(\"ALTER TABLE dim_style ALTER COLUMN StyleID SET NOT NULL;\")\n",
    "spark.sql(\"ALTER TABLE dim_style ADD CONSTRAINT style_pk PRIMARY KEY (StyleID) RELY\")\n",
    "spark.sql(\"ALTER TABLE fct_recipe ADD CONSTRAINT style_fk FOREIGN KEY (StyleID) REFERENCES dim_style(StyleID)\")\n"
   ],
   "id": "9055a2f5591d7ce7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T13:22:14.116165400Z",
     "start_time": "2026-01-30T13:22:00.386953400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark.sql(\"ALTER TABLE books ALTER COLUMN book_id SET NOT NULL;\")\n",
    "spark.sql(\"ALTER TABLE books ADD CONSTRAINT book_id_pk PRIMARY KEY (book_id) RELY\")\n",
    "spark.sql(\"ALTER TABLE tags ALTER COLUMN tag_id SET NOT NULL;\")\n",
    "spark.sql(\"ALTER TABLE tags ADD CONSTRAINT tag_id_pk PRIMARY KEY (tag_id) RELY\")\n",
    "spark.sql(\"ALTER TABLE book_tags ALTER COLUMN goodreads_book_id SET NOT NULL;\")\n",
    "spark.sql(\"ALTER TABLE book_tags ALTER COLUMN tag_id SET NOT NULL;\")\n",
    "spark.sql(\"ALTER TABLE book_tags ADD CONSTRAINT booktag_id_pk PRIMARY KEY (goodreads_book_id, tag_id) RELY\")\n",
    "spark.sql(\"ALTER TABLE book_tags ADD CONSTRAINT booktag_tagid_fk FOREIGN KEY (tag_id) REFERENCES tags(tag_id)\")\n",
    "spark.sql(\"ALTER TABLE book_tags ADD CONSTRAINT booktag_bookid_fk FOREIGN KEY (goodreads_book_id) REFERENCES books(book_id)\")\n",
    "spark.sql(\"ALTER TABLE ratings ALTER COLUMN book_id SET NOT NULL;\")\n",
    "spark.sql(\"ALTER TABLE ratings ALTER COLUMN user_id SET NOT NULL;\")\n",
    "spark.sql(\"ALTER TABLE ratings ADD CONSTRAINT ratingsuser_id_pk PRIMARY KEY (book_id, user_id) RELY\")\n",
    "spark.sql(\"ALTER TABLE ratings ADD CONSTRAINT ratingsuser_id_fk  FOREIGN KEY (book_id) REFERENCES books(book_id) \")\n",
    "\n"
   ],
   "id": "73617a5a3c17b649",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
