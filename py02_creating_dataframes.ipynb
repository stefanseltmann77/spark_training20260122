{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import string\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "from spark_setup_spark3 import get_spark\n",
    "\n",
    "spark = get_spark()\n",
    "\n",
    "# generate some dummy data based on the basic letters and an autoincrement\n",
    "dummy_data = [[i, char] for i, char in enumerate(string.ascii_letters)]\n",
    "# [[0, 'a'], [1, 'b'], ...\n"
   ],
   "id": "be63e6127ef74f13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# data looks like [[0, 'a'], ...]\n",
    "df: DataFrame = spark.createDataFrame(dummy_data)\n",
    "\n",
    "# what's in the dataframe?\n",
    "# look at the human-readable form, the column names are automatically generated\n",
    "df.show()\n",
    "\n",
    "# raw form, 10 rows, the format is still without column names\n",
    "df.take(10)\n",
    "\n",
    "# raw form, take all back to driver\n",
    "df.collect()\n",
    "df.head(5)\n",
    "\n",
    "# schema in human readable form\n",
    "df.printSchema()\n",
    "\n",
    "# schema in code form\n",
    "df.schema\n"
   ],
   "id": "80e707fb03e27eca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create pandas dataframe based on the dummy data\n",
    "pandas_df = pd.DataFrame(dummy_data, columns=['index', 'character'])\n",
    "\n",
    "# cast pandas dataframe to spark dataframe, column names are adopted\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "df.show()\n",
    "\n",
    "# column names are inherited!\n",
    "assert (df.columns == list(pandas_df.columns))  # asserts successfully\n"
   ],
   "id": "4b5b52ad8a7b63d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df: DataFrame = spark.createDataFrame(dummy_data)  # without schema ...\n",
    "df.show()  # ... column names are missing!\n",
    "\n",
    "# create it with simple listing of column names\n",
    "df: DataFrame = spark.createDataFrame(dummy_data, schema=['index', 'character'])\n",
    "df.show()\n",
    "df.columns  # column names are present!\n",
    "\n",
    "# even better:\n",
    "# the most precise way is defining a schema\n",
    "from pyspark.sql.types import StructField, StringType, StructType, ByteType\n",
    "\n",
    "schema = StructType([StructField(\"index\", ByteType(), False),\n",
    "                     StructField(\"character\", StringType(), False)])\n",
    "\n",
    "# A schema collision will FAIL FAST at reading the data!\n",
    "# therefore it is the best way for solid data ingest.\n",
    "df: DataFrame = spark.createDataFrame(dummy_data, schema=schema)\n",
    "df.show()\n",
    "\n",
    "df.columns  # column names are present and the extended schema\n",
    "df.printSchema()\n"
   ],
   "id": "e4f9e7c863967c22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "test_rows = [Row(company=\"ACME\", product=\"Toxic Waste\", product_id=123, stock_value=92.32),\n",
    "             Row(company=\"LUMOS\", product=\"Macro Data\", product_id=345, stock_value=99.32)]\n",
    "df_by_row = spark.createDataFrame(test_rows)\n",
    "df_by_row.show(truncate=False)\n",
    "\n",
    "# Rows are nice as well. They act as a record\n",
    "# get the value per label\n",
    "test_row = test_rows[0]\n",
    "assert test_row.company == 'ACME'\n",
    "# or cast it as a dict\n",
    "assert test_row.asDict() == {'company': 'ACME', 'product': 'Toxic Waste', 'product_id': 123, 'stock_value': 92.32}\n",
    "\n",
    "# all options shown here are useful in order to create test data\n"
   ],
   "id": "1c7f894fc7cdf4ab"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
